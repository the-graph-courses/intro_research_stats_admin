---
title: 'Estimation 1: Sampling Distributions and the Central Limit Theorem'
format:
  html:
    embed-resources: true
---

```{r, echo = F, message = F, warning = F}
# Load packages
if (!require(pacman)) install.packages("pacman")
pacman::p_load(rlang, tidyverse, knitr, here, reactable)

# Source functions
source(here("global/functions/lesson_functions.R"))

# Knitr options
knitr::opts_chunk$set(
  warning = F, message = F,
  class.source = "tgc-code-block", error = T
)

# Set ggplot theme
theme_set(theme_minimal())
```

# **Introduction**

Statistical research begins with two complementary toolkits: descriptive and inferential statistics. Descriptive statistics help us organise, condense, and visualise raw information—for example with tables, charts, or numerical summaries. Inferential statistics, by contrast, use a carefully selected sample to say something about an unseen population. So far in this course, we have focused on descriptive statistics - investigating our data and its distributions before diving into analysis. Today we begin our journey into inferential statistics, starting with **estimation**.

![](images/stats_overview_g4g.png){width="723"}

# **Learning Objectives**

By the end of this less you should be able to

-   Understand why statistical inference is necessary and what makes a good inference.

-   Explain sampling distributions and recognise the role of sampling variation.

-   State and apply the Central Limit Theorem (CLT).

# **Inferential statistics**

Statistical inference is the disciplined process of drawing conclusions about population-level quantities from limited, sample-level data. The population is every individual, unit, or measurement we ultimately care about; the sample is a smaller, manageable subset chosen—ideally at random—from that population.

![](images/pop_vs_sample.png)

Two classical branches of inference are:

1.  **Estimation.** *We estimate unknown population parameters (e.g., the population mean blood-pressure μ) with corresponding sample statistics (e.g., the sample mean* $\bar{x}$).

2.  **Hypothesis testing.** *We formally weigh evidence for or against a claim about a population parameter (e.g., “Men and women have equal mean blood-pressure”).*

![](images/inference_bmi.png)

# **Population and Samples**

We introduced populations and samples in Lesson 1. Here we dig deeper because every inferential tool relies on this distinction.

::: callout-tip
## Terminology Recap

A **sample** is the subset we actually observe. From it we compute sample **statistics**.

-   *Example:* From 1 000 randomly selected adult women in Yaoundé we compute xˉ\bar xxˉ, sss, etc.

A **population** is the full set we wish to understand. Its true **parameters** (μ,σ,…\mu, \sigma,\ldotsμ,σ,…) are almost never known exactly.

-   *Example:* The true mean blood pressure of *all* adults in China is a population parameter.

| Population Parameter (Greek) | Measure | **Sample Statistic (Latin)** |
|:--:|:--:|:--:|
| $\mu$ (mu) | Mean | $\bar{x}$ (x bar) |
| $\sigma^2$ (sigma squared) | Variance | $s^2$ |
| $\sigma$ (sigma) | Standard Deviation | $s$ |
| $N$ | Size | $n$ |
:::

Because complete enumeration is impractical, we almost always substitute statistics for their unknown parameters and then quantify the inevitable uncertainty.

## Population Parameters vs. Sample Statistics

| **Population Parameter** | **Sample Statistic** |
|----|----|
| Describes entire population (usually unknown) | Computed from sample data (known) |
| Denoted by Greek symbols like $\mu$ (population mean) and $\sigma$ (population standard deviation) | Denoted by Latin symbols like $\bar{x}$ (sample mean) and $s$ (sample standard deviation) |
| Estimated by sample statistics | Calculated from sample data and used to estimate population parameters |

------------------------------------------------------------------------

::: callout-note
## Practice Question
:::

------------------------------------------------------------------------

# Sampling Variation

Even with flawless randomisation, **different random samples would produce different statistics.** That wiggle from sample to sample is **sampling variation**.

![](images/clipboard-916810754.png)

## An example of sampling variation

Steps:

1.  Select a random sample of 50 from a population.

2.  Record systolic blood pressure (SBP) measurement for each person in the sample.

3.  Calculate the sample mean blood pressure from those values.

In this plot, each light grey dot represents the full population of over 200,000 individuals in a retrospective study. The y-axis shows the blood pressure of each person

-   Taking a random sample of 50 individuals (blue dots), then calculating the sample mean, we get a value of 120.9 millimeters of mercury.

![](images/sbp_jitter_var_234.png)

-   This sample mean is an estimate of the "true" population mean of 119.1 mmHg.

![](images/sbp_jitter_true.png)

-   But when selecting our random sample, we could have obtained a different set of 50 individuals.

-   Let's plot again. This time we get a different value for the sample mean – 117.5 mmHg.

![](images/sbp_jitter_var_123.png)

-   As we repeat the experiment, we see the mean hop around the target.

![](images/sbp_jitter_var_345.png)

Each different sample we could have obtained would give us a different set of data, and therefore a different sample mean and estimate of the population mean.

# **Sampling distributions**

**Definition.** *The* sampling distribution *of a statistic is the distribution of that statistic over every possible random sample of a given size from the population.*

In practice we approximate it by simulation or theory. The next figures show simulated sampling distributions of the mean SBP for sample sizes 50, 200, 400, 800:

![Sampling distribution of mean SBP in many samples of size 50.](images/sbp_n50.png)

![Sampling distribution of mean SBP in many samples of size 200.](images/sbp_n200.png)

![Sampling distribution of mean SBP in many samples of size 400.](images/sbp_n400.png)

![Sampling distribution of mean SBP in many samples of size 800.](images/sbp_n800.png)

**Notice how larger samples give a *narrower* (less variable) distribution—yet the centre stays at the true mean.** The standard deviation of this sampling distribution is notated SExˉ\text{SE}\_{\bar x}SExˉ​ and called the **standard error**.

# Central Limit Theorem

A remarkable fact—**the Central Limit Theorem (CLT)**—makes inference tractable:

> **For sufficiently large random samples, the sampling distribution of the sample mean is approximately normal, regardless of the shape of the parent population.**

![](images/clt_uniform_evolution.gif){width="699"}

Even if we have only one sample of data, we can use statistical theory to get an idea of what the sampling distribution of a particular statistic looks like.

**Key implications**

-   **Mean of the sampling distribution = population mean (**E\[xˉ\]=μ\mathrm E\[\bar x\]=\muE\[xˉ\]=μ).

-   **Standard deviation =** σ/n\sigma/\sqrt{n}σ/n​, the standard error.

-   **Normality enables confidence intervals and p-values with simple formulas—even for skewed data in the raw scale.**

Remember: *A statistic’s behaviour over repeated sampling is encoded in its sampling distribution. The CLT tells us that, for the mean, this distribution is normal when* nnn is moderately large (often n≥30n\ge 30n≥30 suffices).

::: callout-important
The distribution of a statistic across every possible sample is its **sampling distribution**.

The **Central Limit Theorem (CLT)** states that the sampling distribution of the sample mean is (approximately) a normal distribution.
:::

------------------------------------------------------------------------

# Conclusion

In summary, every inferential statement rests on three pillars: a good sample, an understanding of sampling variation, and the power of the Central Limit Theorem. Master these, and you hold the keys to reproducible, defensible statistical research in R.

# Contributors {.unlisted .unnumbered}

The following team members contributed to this lesson:

`r .tgc_contributors_list(ids = c("joy", "kendavidn"))`

# References {.unlisted .unnumbered}

Some material in this lesson was adapted from the following sources:

-   Irizarry, Rafael A. 2019. *Introduction to Data Science: Statistics and Prediction Algorithms Through Case Studies*. <https://rafalab.dfci.harvard.edu/dsbook-part-2/>.
-   Cannell, Brad, and Melvin Livingston. n.d. *R for Epidemiology*. <https://www.r4epi.com/>.
-   GeeksforGeeks. 2025. "Introduction of Statistics and Its Types." <https://www.geeksforgeeks.org/introduction-of-statistics-and-its-types/>.
-   Starmer, Josh. *StatQuest with Josh Starmer* \[YouTube Channel\]. <https://www.youtube.com/channel/UCtYLUTtgS3k1Fg4y5tAhLbw>.
-   The GRAPH Courses. *FoSSA: Fundamentals of Statistical Software & Analysis*. <https://thegraphcourses.org/courses/fossa/>.
-   University of Miami Libraries. *Introduction to Data Analysis and R*. <https://www.library.miami.edu/data-services/data-analysis.html>.

`r .tgc_license()`
